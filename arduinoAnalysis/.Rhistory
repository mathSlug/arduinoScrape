knn_euclid = mutate(data.frame(knn_Euclidean$fitted.values), type = "Euclidean")
names(knn_euclid) = c("age", "type")
knn_euclid = rbind(knn_euclid, titan4)
knn_mk10 = mutate(data.frame(knn_Minkowski$fitted.values), type = "Minkowski")
names(knn_mk10) = c("age", "type")
knn_mk10 = rbind(knn_mk10, titan4)
plot_data = rbind(titan4, knn_manhat, knn_euclid, knn_mk10)
ggplot(plot_data, aes(x = age, color = type)) + geom_density()
#Only 1 point is selected for each missing value, but the different distance functions
#may find different points to be closer for a given missing value, so they distributions
#look slightly different.
knn_manhattan = kknn(age ~ . , complete_obs, missing_age,
k = trunc(sqrt(nrow(complete_obs))), distance = 1)
knn_Euclidean = kknn(age ~ . , complete_obs, missing_age,
k = trunc(sqrt(nrow(complete_obs))), distance = 2)
knn_Minkowski = kknn(age ~ . , complete_obs, missing_age,
k = trunc(sqrt(nrow(complete_obs))), distance = 10)
knn_manhattan = kknn(age ~ . , complete_obs, missing_age,
k = trunc(sqrt(nrow(complete_obs))), distance = 1)
knn_Euclidean = kknn(age ~ . , complete_obs, missing_age,
k = trunc(sqrt(nrow(complete_obs))), distance = 2)
knn_Minkowski = kknn(age ~ . , complete_obs, missing_age,
k = trunc(sqrt(nrow(complete_obs))), distance = 10)
titan4 = select(mutate(complete_obs, type = "No Impute"), age, type)
knn_manhat = mutate(data.frame(knn_manhattan$fitted.values), type = "Manhattan")
names(knn_manhat) = c("age", "type")
knn_manhat = rbind(knn_manhat, titan4)
knn_euclid = mutate(data.frame(knn_Euclidean$fitted.values), type = "Euclidean")
names(knn_euclid) = c("age", "type")
knn_euclid = rbind(knn_euclid, titan4)
knn_mk10 = mutate(data.frame(knn_Minkowski$fitted.values), type = "Minkowski")
names(knn_mk10) = c("age", "type")
knn_mk10 = rbind(knn_mk10, titan4)
plot_data = rbind(titan4, knn_manhat, knn_euclid, knn_mk10)
ggplot(plot_data, aes(x = age, color = type)) + geom_density()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
library(MASS)
load(cats)
library(MASS)
data(cats)
library(MASS)
data(cats)
head(cats)
library(ggplot2)
ggplot(cats, aes(x = Bwt, y = Hwt, col = Sex)) + geom_point()
model = lm(dist ~ speed, data = cars)
View(model)
model = lm(Hwt ~ Bwt, data = cats)
summary(model)
summary(model)
set.seed(0)
model = lm(Hwt ~ Bwt, data = cats)
summary(model)
model = lm(dist ~ speed, data = cars)
summary(model)
model = lm(dist ~ speed, data = cars)
confint(model)
qqnorm(model$residuals)
qqline(model$residuals)
plot(model)
set.seed(0)
model = lm(Hwt ~ Bwt, data = cats)
summary(model)
plot(model)
plot(model)
plot(model)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
restaurants = read.table("https://s3.amazonaws.com/nycdsabt01/NYC_Restaurants.txt")
View(restaurants)
pairs(~Price+Food+Decor+Service, data = resturants)
pairs(~Price+Food+Decor+Service, data = restaurants)
pairs(~Price+Food+Decor+Service, data = restaurants, col = Location)
?pairs
pairs(~Price+Food+Decor+Service, data = restaurants, col(Location))
pairs(~Price+Food+Decor+Service, data = restaurants, col(restaurants$Location))
pairs(~Price+Food+Decor+Service, data = restaurants, col = restaurants$Location)
######################################################
######################################################
#####[04] Multiple Linear Regression Lecture Code#####
######################################################
######################################################
#####################################################
#####Example using the State Information Dataset#####
#####################################################
help(state.x77)
state.x77 #Investigating the state.x77 dataset.
states = as.data.frame(state.x77) #Forcing the state.x77 dataset to be a dataframe.
#Cleaning up the column names so that there are no spaces.
colnames(states)[4] = "Life.Exp"
colnames(states)[6] = "HS.Grad"
#Creating a population density variable.
states[,9] = (states$Population*1000)/states$Area
colnames(states)[9] = "Density"
#Basic numerical EDA for states dataset.
summary(states)
sapply(states, sd)
cor(states)
#Basic graphical EDA for the states dataset.
plot(states)
#Can we estimate an individual's life expectancy based upon the state in which
#they reside?
#Creating a saturated model (a model with all variables included).
model.saturated = lm(Life.Exp ~ ., data = states)
summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
plot(model.saturated) #Assessing the assumptions of the model.
vif(model.saturated) #Assessing the variance inflation factors for the variables
#in our model.
#Added variable plots for assessing the contribution of each additional variable.
avPlots(model.saturated) #Distinct patterns are indications of good contributions
#to the model; absent patterns usually are pointers to
#variables that could be dropped from the model.
#We note that Illiteracy has a large VIF, an insignificant p-value in the overall
#regression, and no strong distinct pattern in the added-variable plot. What
#happens when we remove it from the model?
model2 = lm(Life.Exp ~ . - Illiteracy, data = states)
summary(model2) #R^2 adjusted went up, model still significant, etc.
plot(model2) #No overt additional violations.
vif(model2) #VIFs all decrease.
#We can compare these two models using a partial F-test using the anova function.
#Here, the first model we supply is the reduced model, and the second is the full
#model.
anova(model2, model.saturated) #The p-value is quite large, indicating that we
#retain the null hypothesis. Recall that the null
#hypothesis is that the slope coefficients of the
#variables in the subset of interest are all 0.
#We retain this hypothesis and conclude that the
#Illiteracy variable is not informative in our
#model; we move forward with the reduced model.
#Let's use the partial F-test to test multiple predictors at once. As compared
#to the saturated model, does the subset of Illiteracy, Area, and Income have
#any effect on our prediction of Life.Exp?
model.full = lm(Life.Exp ~ ., data = states)
model.reduced = lm(Life.Exp ~ . - Illiteracy - Area - Income, data = states)
anova(model.reduced, model.full) #The p-value is quite large; thus, the reduced
#model is sufficient.
#Checking the model summary and assumptions of the reduced model.
summary(model.reduced)
plot(model.reduced)
AIC(model.full,    #Model with all variables.
model2,        #Model with all variables EXCEPT Illiteracy.
model.reduced) #Model with all variables EXCEPT Illiteracy, Area, and Income.
BIC(model.full,
model2,
model.reduced) #Both the minimum AIC and BIC values appear alongside the
#reduced model that we tested above.
#We can use stepwise regression to help automate the variable selection process.
#Here we define the minimal model, the full model, and the scope of the models
#through which to search:
model.empty = lm(Life.Exp ~ 1, data = states) #The model with an intercept ONLY.
model.full = lm(Life.Exp ~ ., data = states) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
library(MASS) #The Modern Applied Statistics library.
#Stepwise regression using AIC as the criteria (the penalty k = 2).
forwardAIC = step(model.empty, scope, direction = "forward", k = 2)
backwardAIC = step(model.full, scope, direction = "backward", k = 2)
bothAIC.empty = step(model.empty, scope, direction = "both", k = 2)
bothAIC.full = step(model.full, scope, direction = "both", k = 2)
#Stepwise regression using BIC as the criteria (the penalty k = log(n)).
forwardBIC = step(model.empty, scope, direction = "forward", k = log(50))
backwardBIC = step(model.full, scope, direction = "backward", k = log(50))
bothBIC.empty = step(model.empty, scope, direction = "both", k = log(50))
bothBIC.full = step(model.full, scope, direction = "both", k = log(50))
#In this case, all procedures yield the model with only the Murder, HS.Grad,
#Frost, and Population variables intact.
#Checking the model summary and assumptions of the reduced model.
summary(forwardAIC)
plot(forwardAIC)
#Predicting new observations.
forwardAIC$fitted.values #Returns the fitted values.
newdata = data.frame(Murder = c(1.5, 7.5, 12.5),
HS.Grad = c(60, 50, 40),
Frost = c(75, 55, 175),
Population = c(7500, 554, 1212))
predict(forwardAIC, newdata, interval = "confidence") #Construct confidence intervals
#for the average value of an
#outcome at a specific point.
predict(forwardAIC, newdata, interval = "prediction") #Construct prediction invervals
#for a single observation's
#outcome value at a specific point.
#####################################
#####Extending Model Flexibility#####
#####################################
tests = read.table("2.a_Test_Scores.txt")
#Basic numerical EDA for states dataset.
summary(tests)
sd(tests$Test.Score)
sd(tests$Hours.Studied)
cor(tests$Test.Score, tests$Hours.Studied)
#Basic graphical EDA for tests dataset.
plot(tests$Hours.Studied, tests$Test.Score)
#############################################
#####Fitting a simple linear regression.#####
#############################################
model.simple = lm(Test.Score ~ Hours.Studied, data = tests)
summary(model.simple) #Investigating the model and assessing some diagnostics.
plot(model.simple)
influencePlot(model.simple)
#Constructing confidence and prediction bands for the scope of our data.
newdata = data.frame(Hours.Studied = seq(1, 3, length = 100))
conf.band = predict(model.simple, newdata, interval = "confidence")
pred.band = predict(model.simple, newdata, interval = "prediction")
plot(tests$Hours.Studied, tests$Test.Score,
xlab = "Hours Studied", ylab = "Test Score",
main = "Simple Linear Regression Model\nTests Dataset")
abline(model.simple, lty = 2)
lines(newdata$Hours.Studied, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Hours.Studied, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Hours.Studied, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Hours.Studied, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
##################################
#####Adding a quadratic term.#####
##################################
model.quadratic = lm(Test.Score ~ Hours.Studied + I(Hours.Studied^2), data = tests)
summary(model.quadratic) #Investigating the model and assessing some diagnostics.
plot(model.quadratic)
influencePlot(model.quadratic)
#Constructing confidence and prediction bands for the scope of our data.
conf.band = predict(model.quadratic, newdata, interval = "confidence")
pred.band = predict(model.quadratic, newdata, interval = "prediction")
plot(tests$Hours.Studied, tests$Test.Score,
xlab = "Hours Studied", ylab = "Test Score",
main = "Quadratic Regression Model\nTests Dataset")
lines(tests$Hours.Studied[order(tests$Hours.Studied)],
model.quadratic$fitted.values[order(tests$Hours.Studied)], lty = 2)
lines(newdata$Hours.Studied, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Hours.Studied, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Hours.Studied, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Hours.Studied, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
##########################
#####Adding a factor.#####
##########################
model.factor = lm(Test.Score ~ Hours.Studied + Gender, data = tests)
summary(model.factor) #Investigating the model and assessing some diagnostics.
plot(model.factor)
influencePlot(model.factor)
col.vec = c(rep("pink", 250), rep("blue", 250))
plot(tests$Hours.Studied, tests$Test.Score, col = col.vec,
xlab = "Hours Studied", ylab = "Test Score",
main = "Linear Regression Model w/ Factor\nTests Dataset")
abline(model.factor$coefficients[1], #Intercept for females.
model.factor$coefficients[2], #Slope for females.
lwd = 3, lty = 2, col = "pink")
abline(model.factor$coefficients[1] + model.factor$coefficients[3], #Intercept for males.
model.factor$coefficients[2], #Slope for males.
lwd = 3, lty = 2, col = "blue")
legend("topleft", c("Female Regression", "Male Regression"),
lwd = 3, lty = 2, col = c("pink", "blue"))
model1 = lm(Price ~ ., data = restaurants)
# A
model1 = lm(Price ~ ., data = restaurants)
str(model1)
# A
model1 = lm(Price ~ Food+Decor+Service+Location, data = restaurants)
str(model1)
# A
Price
model1 = lm(Price ~ Food+Decor+Location, data = restaurants)
str(model1)
# A
# Price = -22 + 1.5*Food + 1.9*Decor + 0.002*Service -2.1*Location
model1 = lm(Price ~ Food+Decor+Service+Location, data = restaurants)
str(model1)
# A
# Price = -22 + 1.5*Food + 1.9*Decor + 0.002*Service -2.1*Location
model1 = lm(Price ~ Food+Decor+Service+Location, data = restaurants)
str(model1$coefficients)
# A
# Price = -22 + 1.5*Food + 1.9*Decor + 0.002*Service -2.1*Location
model1 = lm(Price ~ Food+Decor+Service+Location, data = restaurants)
x = (model1$coefficients)
# A
# Price = -22 + 1.5*Food + 1.9*Decor + 0.002*Service -2.1*Location
model1 = lm(Price ~ Food+Decor+Service+Location, data = restaurants)
x = (model1$coefficients)
# A
# Price = -22 + 1.5*Food + 1.9*Decor + 0.002*Service -2.1*Location
x
model1 = lm(Price ~ Food+Decor+Service+Location, data = restaurants)
str(model1$coefficients)
# A
# Price = -22 + 1.5*Food + 1.9*Decor + 0.002*Service -2.1*LocationWest <- 1 if Loc == West
# B
# A restaurant with zero ratings would cost negative $. Conceptually, it would be so unpleasant that it would need to pay customers to eat there.
# LocationWest is a boolean variable for the catagorical variavle Location, see above.
#Other coeffecients reflect how changes in a variable affect price. They are slopes.
#C
str(model1)
model1 = lm(Price ~ Food+Decor+Service+Location, data = restaurants)
str(model1$coefficients)
# A
# Price = -22 + 1.5*Food + 1.9*Decor + 0.002*Service -2.1*LocationWest <- 1 if Loc == West
# B
# A restaurant with zero ratings would cost negative $. Conceptually, it would be so unpleasant that it would need to pay customers to eat there.
# LocationWest is a boolean variable for the catagorical variavle Location, see above.
#Other coeffecients reflect how changes in a variable affect price. They are slopes.
#C
model1
#C
summary(model1)
plot(model1)
influencePlot(model1)
library(car)
influencePlot(model1)
vif(model1)
avPlots(model1)
model2 = lm(Price ~ Service, data = restaurants)
summary(model2)
model3 = lm(Price ~ Food + Decor, data = restaurants)
model4 = lm(Price ~ Food + Decor + Location)
model3 = lm(Price ~ Food + Decor, data = restaurants)
model4 = lm(Price ~ Food + Decor + Location, data = restaurants)
summary(model3)
summary(model4)
head(GradSchools)
########################################################
GradSchools = read.table("Graduate_Schools.txt")
head(GradSchools)
sapply(GradSchools, sd)
plot(GradSchools, col = GradSchools$admit + 2)
plot(GradSchools, col = GradSchools$admit + 2, position = 'jitter')
#Being naïve at first and fitting a multiple linear regression model.
bad.model = lm(admit ~ gre + gpa + rank, data = GradSchools)
summary(bad.model) #Looks like everything is significant, so
plot(bad.model) #Seve
summary(bad.model$fitted.values)
#Fitting the logistic regression with all variables; the family parameter
#specifies the error distribution and link function to be used. For logistic
#regression, this is binomial.
logit.overall = glm(admit ~ gre + gpa + rank,
family = "binomial",
data = GradSchools)
#-Rank: The change in log odds associated with attending an undergraduate school
#       with prestige of rank 2, 3, and 4, as compared to a school with prestige
#       rank 1, is approximately -0.675, -1.340, and -1.552, respectively, holding
#       all other variables constant.
getwd()
setwd("~/Data_Sci_2k18/arduinoScrape/arduinoAnalysis")
library(shiny)
library(shinydashboard)
library(dplyr)
library(ggplot2)
library(markdown)
library(knitr)
library(DT)
#load data
ard_data = read.csv("data/ard_data.csv", header = TRUE)
ard_data$Title = as.character(ard_data$Title)
ard_data$Description = as.character(ard_data$Description)
ard_data$CommentsNum = as.numeric(ard_data$CommentsNum)
ard_data$Followers = as.numeric(ard_data$Followers)
ard_data$Projects = as.numeric(ard_data$Projects)
ard_data$Respects = as.numeric(ard_data$Respects)
ard_data$Views = as.numeric(ard_data$Views)
ard_data = ard_data %>% mutate(., RespectPerView = round(Respects / Views, 5))
#create boolean columns for each topic
names_topics = ard_data %>% transmute(.,
Title = Title,
Description = Description,
Views = Views,
TopicList = strsplit(as.character(Topics), ','))
all_topics = unlist(c(names_topics$TopicList))
#only want topics that are found on 0.5% or more of projects, otherwise irrelevant
selected_topics_df = data_frame(topics = all_topics, index = 1:length(all_topics)) %>%
group_by(., topics) %>% summarise(., reps = n()) %>%
filter(., reps >= sum(reps)/200)
selected_topics = unique(selected_topics_df$topics)
names_topics[selected_topics] = 0
#to-do: get rid of for loop
for(name in names_topics$Title) {
names_topics[names_topics$Title == name, selected_topics] = selected_topics %in%
unlist(names_topics[names_topics$Title == name, "TopicList"])
}
#add scraped data and topic columns to create useable dataset
use_data = merge(ard_data[, !(names(ard_data) %in% c("Topics"))],
names_topics[, !(names(names_topics) %in% c("TopicList"))],
#include Description and Views to avoid problems w/ duplicate titles
by = c("Title", "Description", "Views"))
predict_data = use_data[,!(names(use_data) %in% c("Respects","Views",
"CommentsNum"))]
View(predict_data)
model1 = lm(RespectPerView ~ Followers + Projects, data = predict_data)
View(model1)
library(shiny)
library(shinydashboard)
library(dplyr)
library(ggplot2)
library(markdown)
library(knitr)
library(DT)
#load data
ard_data = read.csv("data/ard_data.csv", header = TRUE)
ard_data$Title = as.character(ard_data$Title)
ard_data$Description = as.character(ard_data$Description)
ard_data$CommentsNum = as.numeric(ard_data$CommentsNum)
ard_data$Followers = as.numeric(ard_data$Followers)
ard_data$Projects = as.numeric(ard_data$Projects)
ard_data$Respects = as.numeric(ard_data$Respects)
ard_data$Views = as.numeric(ard_data$Views)
ard_data = ard_data %>% mutate(., RespectPerView = round(Respects / Views, 5))
#create boolean columns for each topic
names_topics = ard_data %>% transmute(.,
Title = Title,
Description = Description,
Views = Views,
TopicList = strsplit(as.character(Topics), ','))
all_topics = unlist(c(names_topics$TopicList))
#only want topics that are found on 0.5% or more of projects, otherwise irrelevant
selected_topics_df = data_frame(topics = all_topics, index = 1:length(all_topics)) %>%
group_by(., topics) %>% summarise(., reps = n()) %>%
filter(., reps >= sum(reps)/200)
selected_topics = unique(selected_topics_df$topics)
names_topics[selected_topics] = 0
#to-do: get rid of for loop
for(name in names_topics$Title) {
names_topics[names_topics$Title == name, selected_topics] = selected_topics %in%
unlist(names_topics[names_topics$Title == name, "TopicList"])
}
#add scraped data and topic columns to create useable dataset
use_data = merge(ard_data[, !(names(ard_data) %in% c("Topics"))],
names_topics[, !(names(names_topics) %in% c("TopicList"))],
#include Description and Views to avoid problems w/ duplicate titles
by = c("Title", "Description", "Views"))
predict_data = use_data[,!(names(use_data) %in% c("Respects","Views",
"CommentsNum", "Title",
"Description", "Creator"))]
View(predict_data)
model1 = lm(RespectPerView ~ ., data = predict_data)
calced_vfis = vif(model1)
?vif
library(car)
calced_vfis = vif(model1)
View(calced_vfis)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
model1 = lm(Price ~ Food+Decor+Service+Location, data = restaurants)
restaurants = read.table("https://s3.amazonaws.com/nycdsabt01/NYC_Restaurants.txt")
model1 = lm(Price ~ Food+Decor+Service+Location, data = restaurants)
str(model1$coefficients)
# A
# Price = -22 + 1.5*Food + 1.9*Decor + 0.002*Service -2.1*LocationWest <- 1 if Loc == West
# B
# A restaurant with zero ratings would cost negative $. Conceptually, it would be so unpleasant that it would need to pay customers to eat there.
# LocationWest is a boolean variable for the catagorical variavle Location, see above.
#Other coeffecients reflect how changes in a variable affect price. They are slopes.
#C, D
summary(model1)
#All coeffecients except for Service are significant. The model is significant.
#E
#Little standard deviation of residuals shows a good fit to data.
#F
#61.9% of Price variance explained by model.
vif(model1)
calced_vfis
vif(model1)
View(predict_data)
predict_data = use_data[,!(names(use_data) %in% c("Respects","Views",
"CommentsNum", "Title","Date",
"Description", "Creator"))]
model1 = lm(RespectPerView ~ ., data = predict_data)
calced_vfis = vif(model1)
calced_vfis
runApp()
data.frame(calced_vfis)
runApp()
names(calced_vfis)
runApp()
runApp()
runApp()
runApp()
runApp()
names(calced_vfis)
runApp()
runApp()
runApp()
runApp()
runApp()
View(predict_data)
runApp()
runApp()
runApp()
runApp()
runApp()
predict_data[,!(names(predict_data) == "RespectPerView")]
runApp()
names(predict_data)[!(names(predict_data) == "RespectPerView")]
runApp()
?capture.output
runApp()
?checkboxGroupInput
runApp()
runApp()
runApp()
runApp()
runApp()
?boxCox
runApp()
runApp()
runApp()
runApp()
predict(model1, newdata = predict_data[1,])
predict(model1, newdata = predict_data[1:2,])
View(predict_data)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
